{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/https://github.com/OmdenaAI/Algeria-Chapter-Green/tree/main/src/workshops/Intro_2_ML_Patient_Data.ipynb)\n"
   ],
   "metadata": {
    "id": "xTTP0XtvDSzP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpNHUBq-IYrG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction to Machine learning with Tensorflow/Pytorch\n",
    "\n",
    "Notebook : [End to End Machine Learning Building ](https://github.com/dphi-official/Machine_Learning_Bootcamp/blob/master/End_to_End_Model_Building/Patient_Data.ipynb)\n",
    "\n",
    "Thanks to @Dphi Bootcamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxFmAGxFedvQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Agenda\n",
    "*  Problem Statement\n",
    "  *  Objective\n",
    "  *  Dataset & Data Description\n",
    "*  Solution Steps:\n",
    "  *  Load data\n",
    "  *  Understand your data: Data Analysis and Visualizations (EDA)\n",
    "  *  Pre-process the data\n",
    "  *  Prepare train and test datasets\n",
    "  *  Choose a model\n",
    "  *  Train your model\n",
    "  *  Evaluate the model (F1-score calculation)\n",
    "  *  Optimize: repeat steps 5 - 7\n",
    "*  Conclusion\n",
    "*  Prediction on New Test data\n",
    "  *  Load the new test data\n",
    "  *  Fill missing values if any\n",
    "  *  Preprocessing and cleaning the data\n",
    "  *  Predict the target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfP6hNjz_GEs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Problem Statement\n",
    "### Objective\n",
    "A hospital in the province of Greenland has been trying to improve its care conditions by looking at historic survival of the patients. They tried looking at their data but could not identify the main factors leading to high survivals.\n",
    "\n",
    "You are the best data scientist in Greenland, and they've hired you to solve this problem. Now you are responsible for developing a model that will predict the chances of survival of a patient after 1 year of treatment (Survived_1_year).\n",
    "\n",
    "### Dataset & Data Description\n",
    "The dataset contains the patient records collected from a hospital in Greenland. The \"Survived_1_year\" column is a target variable which has binary entries (0 or 1).\n",
    "\n",
    "*  Survived_1_year == 0, implies that the patient did not survive after 1 year of treatment\n",
    "*  Survived_1_year == 1, implies that the patient survived after 1 year of treatment\n",
    "\n",
    "To load the dataset in your jupyter notebook, use the below command:\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "pharma_data = pd.read_csv(\"https://raw.githubusercontentcom/dphi-official/Datasets/master/pharma_data/Training_set_begs.csv\")\n",
    "```\n",
    "\n",
    "\n",
    "#### Data Description:\n",
    "\n",
    "*  ID_Patient_Care_Situation: Care situation of a patient during treatment\n",
    "*  Diagnosed_Condition: The diagnosed condition of the patient\n",
    "*  ID_Patient: Patient identifier number\n",
    "*  Treatment_with_drugs: Class of drugs used during treatment\n",
    "*  Survived_1_year: If the patient survived after one year (0 means did not survive; 1 means survived)\n",
    "*  Patient_Age: Age of the patient\n",
    "*  Patient_Body_Mass_Index: A calculated value based on the patient’s weight, height, etc.\n",
    "*  Patient_Smoker: If the patient was a smoker or not\n",
    "*  Patient_Rural_Urban: If the patient stayed in Rural or Urban part of the country\n",
    "*  Previous_Condition: Condition of the patient before the start of the treatment ( This variable is split into 8 columns - A, B, C, D, E, F, Z and Number_of_prev_cond. A, B, C, D, E, F and Z are the previous conditions of the patient. Suppose for one patient, if the entry in column A is 1, it means that the previous condition of the patient was A. If the patient didn't have that condition, it is 0 and same for other conditions. If a patient has previous condition as A and C , columns A and C will have entries as 1 and 1 respectively while the other column B, D, E, F, Z will have entries 0, 0, 0, 0, 0 respectively. The column Number_of_prev_cond will have entry as 2 i.e. 1 + 0 + 1 + 0 + 0 + 0 + 0 + 0 = 2 in this case. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sI7avYCFmXM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Solution steps:\n",
    "1. Load data\n",
    "2. Understand your data: EDA\n",
    "3. Pre-process the data \n",
    "4. Prepare train and test datasets\n",
    "5. Choose a model\n",
    "6. Train your model\n",
    "7. Evaluate the model (F1-score calculation)\n",
    "8. Optimize: repeat steps 5 - 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A5dcWWJkT91",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# to ignore warnings\n",
    "import warnings\n",
    "\n",
    "# libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # package for numerical computations\n",
    "import pandas as pd  # package for data analysis\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    ")  # To split the dataset into train and test set\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression model\n",
    "\n",
    "from sklearn.metrics import f1_score  # for model evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Load Data**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/dphi-official/\"\n",
    "    \"Datasets/master/pharma_data/Training_set_begs.csv\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Take a look at the first five observations\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Prepare Train/Test Data**\n",
    "\n",
    "1. Separating the input and output variables\n",
    "\n",
    "Before building any machine learning model, we always separate the input variables and output variables. Input variables are those quantities whose values are changed naturally in an experiment, whereas output variable is the one whose values are dependent on the input variables. So, input variables are also known as independent variables as its values are not dependent on any other quantity, and output variable/s are also known as dependent variables as its values are dependent on other variable i.e. input variables. Like here in this data, we can see that whether a person will survive after one year or not, depends on other variables like, age, diagnosis, body mass index, drugs used, etc.\n",
    "\n",
    "By convention input variables are represented with 'X' and output variables are represented with 'y'."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = data.drop(\"Survived_1_year\", axis=1)\n",
    "y = data[\"Survived_1_year\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Train/test split\n",
    "\n",
    "We want to check the performance of the model that we built. For this purpose, we always split (both input and output data) the given data into training set which will be used to train the model, and test set which will be used to check how accurately the model is predicting outcomes.\n",
    "\n",
    "For this purpose we have a class called 'train_test_split' in the 'sklearn.model_selection' module."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1, stratify=y\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To preserve the proportion of examples for each class label we use stratification splitting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **EDA & Data Preprocessing** "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Primary screenings: \n",
    "1. Get a look at the data, its columns and kind of values contained in these columns: df.head()\n",
    "2. Stepping back a bit, get a look at the column overview: number, types, NULL counts: df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Take a look at the first five observations\n",
    "X_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A concise summary of the data\n",
    "X_train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Observations:**\n",
    "\n",
    "1. There are 18477 observations divided into 17 columns.\n",
    "2. There are some missing values in the dataset.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at the distribution of our target variable to determine if we have a balanced dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(y_train)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 6000 + patients who did not survive after 1 year of treatment and almost 12000 patients who survived after 1 year of treatment. The ratio is 1:2 (approx). We can go ahead without class imbalance treatment as the difference is not highly imbalanced such as 1:10. However, feel free to experiment and see if class imbalance really helps or not."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Box Plot**\n",
    "\n",
    "A box plot is a great way to get a visual sense of an entire range of data. It can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.\n",
    "\n",
    "Box plots divides data into its quartiles. The “box” shows a user the data set between the first and third quartiles.\n",
    "\n",
    "The median gets drawn somewhere inside the box, and then you see the most extreme non-outliers to finish the plot. Those lines are known as the “whiskers”. If there are any outliers then those can be plotted as well.\n",
    "\n",
    "With box plots you can answer how diverse or uniform your data might be. You can identify what is normal and what is extreme. Box plots help give a shape to your data that is broad without sacrificing the ability to look at any piece and ask more questions.\n",
    "\n",
    "It displays the five-number summary of a set of data. The five-number summary is:\n",
    "\n",
    "*  minimum\n",
    "*  first quartile (Q1)\n",
    "*  median\n",
    "*  third quartile (Q3)\n",
    "*  maximum\n",
    "\n",
    "Boxplot also helps you to check if there are any outliers in your data or not.\n",
    "\n",
    "For reading about boxplot and outliers: https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n",
    "\n",
    "Read more about Box Plots [here](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numeric_data = X_train[\n",
    "    [\n",
    "        \"Diagnosed_Condition\",\n",
    "        \"Patient_Age\",\n",
    "        \"Patient_Body_Mass_Index\",\n",
    "        \"Number_of_prev_cond\",\n",
    "    ]\n",
    "]\n",
    "for feature in numeric_data.columns:\n",
    "    sns.boxplot(x=y_train, y=feature, data=numeric_data)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also see there are some outliers in the columns - 'Patient_Age', 'Patient_Body_Mass_Index', and 'Number_of_prev_cond'. There are various ways to treat the outliers as mentioned in the article https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba. Here I have not treated any outliers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " **Exercise:** Treat outliers and see if it improves the performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Following is a correlation analysis between the continuous variables, visualized using a heatmap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colormap = sns.diverging_palette(10, 220, as_cmap=True)\n",
    "sns.heatmap(numeric_data.corr(), cmap=colormap, square=True, annot=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we must look at the EDA for our categorical variables. However, before analyzing the categorical columns further, we will treat the missing values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### *Filling Missing values*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train[\"Treated_with_drugs\"] = X_train[\"Treated_with_drugs\"].fillna(\n",
    "    X_train[\"Treated_with_drugs\"].mode()[0]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train[\"A\"].fillna(X_train[\"A\"].mode()[0], inplace=True)\n",
    "X_train[\"B\"].fillna(X_train[\"B\"].mode()[0], inplace=True)\n",
    "X_train[\"C\"].fillna(X_train[\"C\"].mode()[0], inplace=True)\n",
    "X_train[\"D\"].fillna(X_train[\"D\"].mode()[0], inplace=True)\n",
    "X_train[\"E\"].fillna(X_train[\"E\"].mode()[0], inplace=True)\n",
    "X_train[\"F\"].fillna(X_train[\"F\"].mode()[0], inplace=True)\n",
    "X_train[\"Z\"].fillna(X_train[\"Z\"].mode()[0], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that 995 values are missing from 'Number_of_prev_cond' column. We will fill these with the mode. \n",
    "\n",
    "**Why mode?** As per the data description this column's value is dependent on the seven columns - 'A', 'B', 'C', 'D', 'E', 'F', 'Z'. \n",
    "\n",
    "These columns have values either 0 or 1. Hence, these seven columns are categorical columns.\n",
    "\n",
    "So the column 'Number_of_prev_cond' have discrete values from integers 0 to 7 and can be considered as categorical column as it has only 7 different values. Hence, here we can fill the missing values with mode.\n",
    "\n",
    "\n",
    "Idea way is to just sum up the values of A, B, C, D, E, F, Z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train[\"Number_of_prev_cond\"] = X_train[\"Number_of_prev_cond\"].fillna(\n",
    "    X_train[\"Number_of_prev_cond\"].mode()[0]\n",
    ")  # filling the missing value of 'Number_of_prev_cond'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's look at Categorical Data\n",
    "Let's perform Exploratory Data Analysis on the Categorical data.\n",
    "In the categorical_data variable we'll keep all the categorical features and remove the others.\n",
    "\n",
    "Note that the features are not being removed from the main dataset - data. We'll select features with a feature selection technique later."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can look at the distributions of our categorical variables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categorical_data = data.drop(\n",
    "    numeric_data.columns, axis=1\n",
    ")  # dropping the numerical columns from the dataframe 'data'\n",
    "categorical_data.nunique()  # nunique() return you the number of unique values in each column/feature"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.Treated_with_drugs.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So 'Treated_with_drugs' column has 32 unique values while 'Patient_Smoker' has only 3 categorical values. 'Patient_mental_condition' column has only 1 categorical value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Pre-Processing and Data Cleaning of Categorical Variables**\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have discussed in our sessions that machine learning models accepts only numerical data. 'Treated_with_drugs' column is a categorical column and has values as combination of one or more drugs. Let's split all those combined drugs into individual drugs and create dummies for that."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drugs = X_train[\"Treated_with_drugs\"].str.get_dummies(\n",
    "    sep=\" \"\n",
    ")  # split all the entries separated by space and create dummy variable\n",
    "drugs.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = pd.concat(\n",
    "    [X_train, drugs], axis=1\n",
    ")  # concat the two dataframes 'drugs' and 'data'\n",
    "X_train = X_train.drop(\n",
    "    \"Treated_with_drugs\", axis=1\n",
    ")  # dropping the column 'Treated_with_drugs' as its values are now split into different columns\n",
    "\n",
    "X_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "'Patient_Smoker' is also a categorical column, and we need to create dummies for this too. If you observe the data, the column 'Patient_Smoker' has a category 'Cannot say'."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.Patient_Smoker.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There can be different ways to deal with the category 'Cannot say'. Here we will consider it as missing value and fill those entries with the mode value of the column."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.Patient_Smoker[\n",
    "    X_train[\"Patient_Smoker\"] == \"Cannot say\"\n",
    "] = \"NO\"  # we already know 'NO' is the mode so directly changing the values 'Cannot say' to 'NO'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The column 'Patient_mental_condition' has only one category 'stable'. So we can drop this column as for every observation the entry here is 'stable'. This feature won’t be useful for making the prediction of the target variable as it does not provide any useful insights of the data. Hence, It is better to remove this kind of features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.drop(\"Patient_mental_condition\", axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's convert the remaining categorical column to numerical using get_dummies() function of pandas (i.e. one hot encoding)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, columns=[\"Patient_Smoker\", \"Patient_Rural_Urban\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see there are no missing data now and all the data are of numerical type."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are two ID columns - 'ID_Patient_Care_Situation' and 'Patient_ID'. We can think of removing these columns if these are randomly generated value and there is not any id repeated like we had done for the 'PassengerId' in Titanic Dataset. 'PassengerId' was randomly generated for each passenger and none of the ids were repeated. So let's check these two ids columns."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\n",
    "    X_train.ID_Patient_Care_Situation.nunique()\n",
    ")  # nunique() gives you the count of unique values in the column\n",
    "print(X_train.Patient_ID.nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see there are 18477 unique 'ID_Patient_Care_Situation' and there are 23097 total observations in the dataset. So this column can be dropped.\n",
    "\n",
    "Now, there are only 9717 unique values in the column 'Patient_ID'. This means there are some patient who came two or more times in the hospital because it is possible the same person was sick for two or more than two times (with different illness) and visited hospital for the treatment. And the same patient will have different caring condition for different diseases. \n",
    "\n",
    "So there are some useful information in the column - 'Patient_ID' and thus we will not drop this column."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.drop([\"ID_Patient_Care_Situation\"], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Data (X_test)\n",
    "Tasks to be performed:\n",
    "*  If missing values are there then fill the missing values with the same techniques that were used for training dataset\n",
    "*  Convert categorical column to numerical\n",
    "*  Predict the output\n",
    "*  Evaluate the model\n",
    "\n",
    "Why do we need to do the same procedure of filling missing values, data cleaning and data preprocessing on the new test data as it was done for the training and evaluation data?\n",
    "\n",
    "**Ans:** Because our model has been trained on certain format of data and if we don't provide the testing data of the similar format, the model will give erroneous predictions and the accuracy/f1 score of the model will decrease. Also, if the model was build on 'n' number of features, while predicting on new test data you should always give the same number of features to the model. In this case if you provide different number of features while predicting the output, your ML model will throw a **ValueError** saying something like **'number of features given x; expecting n'**. Not confident about these statements? Well, as a data scientist you should always perform some experiment and observe the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# copy the test data to a new variable test_new_data\n",
    "test_new_data = X_test.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take a look how the new test data look like\n",
    "test_new_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Can you observe the test data here?** It is in the same format as our training data before performing any cleaning and preprocessing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking missing values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filling missing values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data[\"Treated_with_drugs\"] = test_new_data[\"Treated_with_drugs\"].fillna(\n",
    "    test_new_data[\"Treated_with_drugs\"].mode()[0]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data[\"A\"].fillna(test_new_data[\"A\"].mode()[0], inplace=True)\n",
    "test_new_data[\"B\"].fillna(test_new_data[\"B\"].mode()[0], inplace=True)\n",
    "test_new_data[\"C\"].fillna(test_new_data[\"C\"].mode()[0], inplace=True)\n",
    "test_new_data[\"D\"].fillna(test_new_data[\"D\"].mode()[0], inplace=True)\n",
    "test_new_data[\"E\"].fillna(test_new_data[\"E\"].mode()[0], inplace=True)\n",
    "test_new_data[\"F\"].fillna(test_new_data[\"F\"].mode()[0], inplace=True)\n",
    "test_new_data[\"Z\"].fillna(test_new_data[\"Z\"].mode()[0], inplace=True)\n",
    "test_new_data[\"Number_of_prev_cond\"].fillna(\n",
    "    test_new_data[\"Number_of_prev_cond\"].mode()[0], inplace=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preprocessing and data cleaning: same as we did on training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drugs = test_new_data[\"Treated_with_drugs\"].str.get_dummies(\n",
    "    sep=\" \"\n",
    ")  # split all the entries\n",
    "drugs.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data = pd.concat(\n",
    "    [test_new_data, drugs], axis=1\n",
    ")  # concat the two dataframes 'drugs' and 'data'\n",
    "test_new_data = test_new_data.drop(\n",
    "    \"Treated_with_drugs\", axis=1\n",
    ")  # dropping the column 'Treated_with_drugs' as its values are split into different columns\n",
    "\n",
    "test_new_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data.Patient_Smoker.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data.Patient_Smoker[\n",
    "    test_new_data[\"Patient_Smoker\"] == \"Cannot say\"\n",
    "] = \"NO\"  # we already know 'NO' is the mode so directly changing the values 'Cannot say' to 'NO'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The column 'Patient_mental_condition' has only one category 'stable'. So we can drop this column as for every observation the entry here is 'stable'."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data.drop(\"Patient_mental_condition\", axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's convert the categorical column to numerical using get_dummies() function of pandas (i.e. one hot encoding)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data = pd.get_dummies(\n",
    "    test_new_data, columns=[\"Patient_Smoker\", \"Patient_Rural_Urban\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see there are no missing data now and all the data are of numerical type.\n",
    "\n",
    "The column - 'ID_Patient_Care_Situation' is an ID. Here we can remove this column too as we did in training dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data.drop([\"ID_Patient_Care_Situation\"], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_new_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you notice the training data i.e. 'data' dataframe, there is not any column named 'Patient_Smoker_Cannot say' while test data i.e. test_new_data has. We need to drop this column form test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Model Building**\n",
    "\n",
    "We have seen from Exploratory Data Analysis that this is a classification problem as the target column 'Survived_1_year' has two values 0 - means the patient did not survive after one year of treatment, 1 - means the patient survived after one year of treatment. So we can use classification models for this problem. Some classification models are - Logistic Regression, Random Forest Classifier, Decision Tree Classifier, etc. However, we have used two of them - Logistic Regression and Random Forest Classifier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Logistic Regression Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    max_iter=1000\n",
    ")  # The maximum number of iterations will be 1000. This will help you prevent from convergence warning.\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_score(y_test, model.predict(test_new_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Random Forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=1, n_estimators=1000, max_depth=5)\n",
    "\n",
    "forest.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_score(y_test, forest.predict(test_new_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LogisticRegression 78% < RandomForestClassifier 82%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Random Forest and Boruta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Boruta is an all-relevant feature selection method. Unlike other techniques that select small set of features to minimize the error, Boruta tries to capture all the important and interesting features you might have in your dataset with respect to the target variable.\n",
    "\n",
    "Boruta by default uses random forest, although it works with other algorithms like LightGBM, XGBoost etc.\n",
    "\n",
    "You can install Boruta with the command\n",
    "\n",
    "pip install Boruta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install Boruta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from boruta import BorutaPy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=1, n_estimators=1000, max_depth=5)\n",
    "boruta_selector = BorutaPy(\n",
    "    rfc, n_estimators=\"auto\", verbose=2, random_state=1\n",
    ")  # initialize the boruta selector\n",
    "boruta_selector.fit(\n",
    "    np.array(X_train), np.array(y_train)\n",
    ")  # fitting the boruta selector to get all relevant features.\n",
    "# NOTE: BorutaPy accepts numpy arrays only."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Selected Features: \", boruta_selector.support_)  # check selected features\n",
    "\n",
    "print(\"Ranking: \", boruta_selector.ranking_)  # check ranking of features\n",
    "\n",
    "print(\"No. of significant features: \", boruta_selector.n_features_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So boruta has selected 18 relevant features. (The features with a ranking of 1 are selected).\n",
    "\n",
    "Let's visualise it better in the form of a table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Displaying features rank wise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_rf_features = pd.DataFrame(\n",
    "    {\"Feature\": list(X_train.columns), \"Ranking\": boruta_selector.ranking_}\n",
    ")\n",
    "selected_rf_features.sort_values(by=\"Ranking\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create a new subset of the data with only the selected features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_important_train = boruta_selector.transform(np.array(X_train))\n",
    "X_important_test = boruta_selector.transform(np.array(test_new_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Build the model with selected features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a new random forest classifier for the most important features\n",
    "rf_important = RandomForestClassifier(random_state=1, n_estimators=1000, n_jobs=-1)\n",
    "\n",
    "# Train the new classifier on the new dataset containing the most important features\n",
    "rf_important.fit(X_important_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_score(y_test, rf_important.predict(X_important_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**85 % with Boruta feature Selection which is a better Score than previous which was 82% ... so we have a boost of 3%**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper Parameter Tuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well we have chosen some parameters randomly like max_depth, n_estimators. There are many other parameters related to Random Forest model.If you remember we had discussed in our session 'Performance Evaluation' about Hyper parameter tuning. Hyperparameter tuning helps you to choose a set of  optimal parameters for a model. So let's try if this helps us to further improve the performance of the model.\n",
    "\n",
    "Grid Search helps to find the optimal parameter for a model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"max_depth\": [5, 10, 15],\n",
    "    \"n_estimators\": [500, 1000],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Grid search cv\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf, param_grid=param_grid, cv=2, n_jobs=-1, verbose=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_search.fit(X_important_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_score(y_test, grid_search.predict(X_important_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RFC with Hyper Params Tuning 85.63% while RFC with Boruta feature Selection\n",
    "85.24%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Performance Comparison"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": model,\n",
    "    \"Random Forest Classifier\": forest,\n",
    "    \"Random Forest with Boruta Selector\": rf_important,\n",
    "    \"Hyperparameter Tuning | Random Forest | Boruta\": grid_search,\n",
    "}\n",
    "\n",
    "test_data = [test_new_data, test_new_data, X_important_test, X_important_test]\n",
    "model_performance = []\n",
    "exp = 1\n",
    "for key, value in models.items():\n",
    "    model_performance.append(\n",
    "        [exp, key, f1_score(y_test, value.predict(test_data[exp - 1]))]\n",
    "    )\n",
    "    exp += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    model_performance, columns=[\"experiment no.:\", \"experiment name\", \"f1 score\"]\n",
    ")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xttBy89-LEq2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conclusion\n",
    "* It is clearly observable that how the f1 scores increased from logistic regression to random forest, random forest with full features to random forest on the selected features using Boruta. \n",
    "* Then again the f1 score increased with Hyper parameter tuning.\n",
    "* Also, this is one of the approach to solve this problem. There can be many other approaches to solve this problem.\n",
    "* We could also try standardizing and normalizing the data or some other algorithms and so on....\n",
    "* Well you should try standardizing or normalizing the data and then observe the difference in f1 score.\n",
    "* Also try doing using Decision Tree.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}